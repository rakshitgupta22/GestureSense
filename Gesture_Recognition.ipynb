{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gcQiN-f1-itE",
        "qzgb6lhffhQc",
        "_p6gY7Lhf0Kc",
        "9TcodFm_gPXN",
        "bzhsIzLimG7R"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakshitgupta22/GestureSense/blob/main/Gesture_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqs3Z5_z8QYJ"
      },
      "source": [
        "## We are working on classification of Warn, Turn and Circle gestures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWJ8JBVZcZ8v"
      },
      "source": [
        "## Project Result Outline\n",
        "- [EDA and Data Preparation (selected milestone 1 content)](#EDA)\n",
        "\t- [Shift the origin of the data to the center of the body](#Shifttheorigin)\n",
        "\t- [Evaluating polar angle, angular velocity, and angular acceleration of from (x,y)](#angle-data)\n",
        "\t- [Store/write the polar angle velocity and polar angle acceleration data in the 3rd and the 4th component of each dataset.](#storing-angle-data)\n",
        "- [Classification with LSTM (selected milestone 2 content)](#LSTM-classification)\n",
        "\t- [Data preparation for LSTM by Keras](#Keras-data)\n",
        "\t- [Split data to train(80%) and test(20%)](#Train-test-split)\n",
        "\t- [LSTM model performance with different configurations](#lstm-model)\n",
        "\t- [LSTM model performance with different datasets](#differnt-datasets)\n",
        "\t- [Reasons for the varying performance due to different model configurations or datasets](#performance-reasons)\n",
        "\n",
        "\n",
        "- [Analyzing an optimal classification model(milestone 3)](#optimal-model)\n",
        "\n",
        "\t- [Optimal model with GridSearchCV](#GridSearchCV)\n",
        "\t- [confusion matrix and classification report for analyzing the binary classification result](#confusion-matrix-and-classification-report)\n",
        "\n",
        "\t- [Best ML algorithm and baseline accuracy discssion](#baseline-accuracy)\n",
        "\t- [CNN-LSTM model](#CNN-LSTM-model)\n",
        "\t- [Summary of what factor combination yields best prediction result](#factor-summary)\n",
        "\t- [Refine/repeat/discussion of the results](#result-discssion)\n",
        "  \n",
        "  \n",
        "- [Bonus item B1](#bonus-B1)\n",
        "\t- [4 gestures recognition](#4-gestures)\n",
        "\t- [9 gestures recognition](#9-gestures)\n",
        "- [Bonus item B2](#bonus-B2)\n",
        "\t- [Better results with other ML model](#Other-model)\n",
        "\t- [Better results with other data model](#Other-data)\n",
        "- [Additional related results not shown above](#additional_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08quaphDONBR"
      },
      "source": [
        "**Load packages:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikemqyqSu_xQ"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxY-BsXTGEUi",
        "outputId": "6c7db3a0-769f-4cd9-f9a7-e652dbe70764"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsI1G36HHj27"
      },
      "source": [
        " You may use [this chrome app](https://script.google.com/macros/s/AKfycbxbGNGajrxv-HbX2sVY2OTu7yj9VvxlOMOeQblZFuq7rYm7uyo/exec) to copy the CS256Project folder that I shared with you to your own drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM2o2IwXIQgY",
        "outputId": "13c6b88c-e277-41f4-d296-a5e5c60b16f3"
      },
      "source": [
        "root = \"./drive/My Drive/CS256Project/data\"\n",
        "\n",
        "(os.path.exists(root)) #Checking if the data paths indeed exist and are valid."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJMH4qG1qqOa"
      },
      "source": [
        "- Import libraries and models needed for this work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJTN-_ZRqqOb"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd\n",
        "import colorsys\n",
        "import os\n",
        "from PIL import Image, ImageDraw\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from itertools import combinations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vCsR-CfNzVI"
      },
      "source": [
        "<a name=\"EDA\"></a>\n",
        "## EDA and Data Preparation (selected milestone 1 content)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQGFhJ_dnUqE"
      },
      "source": [
        "Everyone must put the copied folder in the exact path shown below so you collaborate and get graded easily.   \n",
        "**No project score will be given for not following this folder configuration**    \n",
        "(Our grader is *not reponsible to figure out your own* perferred Google *folder* configuration)   \n",
        "Basically, you simply need to use the chrome app above to copy the entire CS256Project folder to your Google doc root folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL4hceLhnUqE",
        "outputId": "53ee4450-afe5-44a8-c7a7-425242b7bcf2"
      },
      "source": [
        "gestures_data_path = \"./drive/My Drive/CS256Project/data/\"\n",
        "gestures_2d_path = os.path.join(gestures_data_path, \"gestures_basic_d2\")\n",
        "\n",
        "(os.path.exists(gestures_data_path),\n",
        " os.path.exists(gestures_2d_path)\n",
        ") #Checking if the data paths indeed exist and are valid."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gestures = ['circle', 'turn']"
      ],
      "metadata": {
        "id": "Hv4vdG3H05-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic_69BeLOumc"
      },
      "source": [
        "## Exploring data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbbA9t2xOgvs",
        "outputId": "be353d2d-fe78-492a-97b7-94f8f1fba5d8"
      },
      "source": [
        "# Sample Datapoint Properties\n",
        "class_label = \"abort\"\n",
        "video_id = \"1_0\"\n",
        "n_frames = 24\n",
        "# Extract a sample data and see what is in it\n",
        "sample_2d_data_path = os.path.join(gestures_2d_path, class_label, f\"{class_label}_{video_id}.mp4.npz\")\n",
        "data = np.load(sample_2d_data_path, allow_pickle=True)\n",
        "# What does the data contain?\n",
        "data.files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['boxes', 'segments', 'keypoints', 'metadata']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWI5m85rz02r",
        "outputId": "e37ca746-ce8e-4e9a-cec8-87956e8a50d0"
      },
      "source": [
        "data['keypoints'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E46xYYXld8kd"
      },
      "source": [
        "### keypoints is where it stores the (x,y) coordindate values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5X3iZzgdWac",
        "outputId": "26d96e3f-d184-4a79-d8a4-fbcfef590c25"
      },
      "source": [
        "coords = data['keypoints']\n",
        "coords[0,1].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 4, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nfr = data['keypoints'].shape[0] # number of frames\n",
        "coords = data['keypoints']\n",
        "coords[0,1].shape"
      ],
      "metadata": {
        "id": "-0mSsMbcy-at",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c514b1-feac-4452-ce5f-27de5ace5ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 4, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcQiN-f1-itE"
      },
      "source": [
        "### There are 17 labeled joints as generated from a research algorithm based on the human body image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3wewhLL_x_I"
      },
      "source": [
        "**The joint is indexed as follows**.   \n",
        "0  - Nose   \n",
        "1  - left eye   \n",
        "2  - Right eye   \n",
        "3  - left ear   \n",
        "4  - Right ear   \n",
        "5  - left shoulder   \n",
        "6  - Right shoulder   \n",
        "7  - left elbow   \n",
        "8  - Right elbow   \n",
        "9  - left wrist   \n",
        "10  - Right wrist   \n",
        "11  - left hip   \n",
        "12  - Right hip   \n",
        "13  - left knee   \n",
        "14  - Right knee   \n",
        "15  - left Ankle   \n",
        "16  - Right Ankle   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzZvPVy146GJ"
      },
      "source": [
        "<img alt=\"17 labeled joints\" src=\"https://drive.google.com/uc?export=view&id=1fsCUlc7fetOIRpyL6_emdeFPzu1zJUHn\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzgb6lhffhQc"
      },
      "source": [
        "### (x,y) coordinate of the first frame in the given sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQdf9Q3ZeP88",
        "outputId": "2ee38e83-c3b5-4dab-830e-e1e5feb40ade"
      },
      "source": [
        "coords[0][1][0][:,16]\n",
        "# The first two components are (x,y) coordinate values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.5641040e+02, 4.1656277e+02, 0.0000000e+00, 1.2290478e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p6gY7Lhf0Kc"
      },
      "source": [
        "### Let's take a tranponse so we can access the (x,y) coordindate easier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-Ln5t0zO2YN",
        "outputId": "e9764cf0-20dd-4a27-ca7c-c375877d700a"
      },
      "source": [
        "# [0][1][0] because... [frame_id][always 1][0 to remove extra dimension]\n",
        "# transpose(1, 0) to make it shape (17, 4) so first index is keypoint index\n",
        "coords_0 = coords[0][1][0].transpose(1, 0)\n",
        "coords_0.shape, coords_0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17, 4),\n",
              " array([[2.9177484e+02, 1.8652383e+02, 0.0000000e+00, 1.1042018e+00],\n",
              "        [3.0136520e+02, 1.7873605e+02, 0.0000000e+00, 1.5192554e+00],\n",
              "        [2.8278391e+02, 1.7873605e+02, 0.0000000e+00, 1.7079070e+00],\n",
              "        [3.1694955e+02, 1.8592477e+02, 0.0000000e+00, 1.2796223e+00],\n",
              "        [2.7139536e+02, 1.8472665e+02, 0.0000000e+00, 1.4278147e+00],\n",
              "        [3.4332303e+02, 2.4822697e+02, 0.0000000e+00, 2.5107551e-01],\n",
              "        [2.4322366e+02, 2.4523167e+02, 0.0000000e+00, 2.3285715e-01],\n",
              "        [3.4931699e+02, 3.2550568e+02, 0.0000000e+00, 4.2901537e-01],\n",
              "        [2.3183511e+02, 3.2670383e+02, 0.0000000e+00, 4.9821147e-01],\n",
              "        [3.5710916e+02, 4.0937405e+02, 0.0000000e+00, 4.2143461e-01],\n",
              "        [2.2763934e+02, 4.0398254e+02, 0.0000000e+00, 3.7857029e-01],\n",
              "        [3.2414230e+02, 4.0278439e+02, 0.0000000e+00, 7.1696095e-02],\n",
              "        [2.6000681e+02, 3.9919003e+02, 0.0000000e+00, 1.0201875e-01],\n",
              "        [3.2414230e+02, 4.7467157e+02, 0.0000000e+00, 1.6252953e-01],\n",
              "        [2.5341342e+02, 4.7467157e+02, 0.0000000e+00, 1.8560408e-01],\n",
              "        [3.3253387e+02, 4.2195435e+02, 0.0000000e+00, 8.8789798e-03],\n",
              "        [2.5641040e+02, 4.1656277e+02, 0.0000000e+00, 1.2290478e-02]],\n",
              "       dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coords_ = []\n",
        "for i in range(nfr):\n",
        "  coords_.append(coords[i][1][0].transpose(1, 0))\n",
        "\n",
        "coords_ = np.array(coords_)\n",
        "coords_.shape"
      ],
      "metadata": {
        "id": "JkPsPVZJy5Zp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84028937-3acc-4ba7-a828-e7b77500132f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 17, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TcodFm_gPXN"
      },
      "source": [
        "<a name=\"Shifttheorigin\"></a>\n",
        "## Shift the origin to the nose and recalculate the new (x,y) coordinate for all datasets and\n",
        "# overwrite the original (x,y) values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGOwc0uegeTK"
      },
      "source": [
        "new_origin_joint_num = 0 # Nose\n",
        "\n",
        "for i in range(nfr):\n",
        "  nose_x, nose_y = coords_[i][new_origin_joint_num][:2] # Get the (x,y) coordinates of the new origin\n",
        "  coords_[i,:,0] -= nose_x # Shift x-coordinates\n",
        "  coords_[i,:,1] -= nose_y # Shift y-coordinates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(coords_)):\n",
        "  coords[i][1][0] = coords_[i].transpose(1, 0)"
      ],
      "metadata": {
        "id": "KbnsehtIy2C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Function : 3D to 2D"
      ],
      "metadata": {
        "id": "bzhsIzLimG7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transpose_result(coords):\n",
        "  transposed_coords = []\n",
        "  for i in range(len(coords)):\n",
        "    transposed_coords.append(coords[i][1][0].transpose(1, 0).copy())\n",
        "  return transposed_coords"
      ],
      "metadata": {
        "id": "bzprCPtgmHPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vwkC9-JgkRl"
      },
      "source": [
        "<a name=\"angle-data\"></a>\n",
        "# Calculate the polar angle velocity and the polar angle acceleration based on the transformed (x,y) from above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0GXhiJbg6i6"
      },
      "source": [
        "def cal_vel_acc(gesture_name, sample_number):\n",
        "  gestures_data_path = \"./drive/My Drive/CS256Project/data/\"\n",
        "  gestures_2d_path = os.path.join(gestures_data_path, \"gestures_basic_d2\")\n",
        "\n",
        "  if not (os.path.exists(gestures_data_path) and os.path.exists(gestures_2d_path)):\n",
        "    print(\"Error: Can not access google drive folder\")\n",
        "    return -1\n",
        "\n",
        "  sample_2d_data_path = os.path.join(gestures_2d_path, gesture_name, f\"{gesture_name}_{sample_number}.mp4.npz\")\n",
        "  data = np.load(sample_2d_data_path, allow_pickle=True)\n",
        "\n",
        "  joint_lables = {\n",
        "    0: \"nose\",\n",
        "    1: \"left eye\",\n",
        "    2: \"Right eye\",\n",
        "    3: \"left ear\",\n",
        "    4: \"Right ear\",\n",
        "    5: \"left shoulder\",\n",
        "    6: \"right shoulder\",\n",
        "    7: \"left elbow\",\n",
        "    8: \"Right elbow\",\n",
        "    9: \"left wrist\",\n",
        "    10: \"Right wrist\",\n",
        "    11: \"left hip\",\n",
        "    12: \"Right hip\",\n",
        "    13: \"left knee\",\n",
        "    14: \"Right knee\",\n",
        "    15: \"left ankle\",\n",
        "    16: \"Right ankle\"\n",
        "  }\n",
        "\n",
        "  nfr = data['keypoints'].shape[0] # number of frames\n",
        "  coords = data['keypoints']\n",
        "\n",
        "  coords_ = []\n",
        "  for i in range(nfr):\n",
        "    coords_.append(coords[i][1][0].transpose(1, 0))\n",
        "\n",
        "  coords_ = np.array(coords_)\n",
        "\n",
        "  new_origin_joint_num = 0 # Nose\n",
        "\n",
        "  for i in range(nfr):\n",
        "    nose_x, nose_y = coords_[i][new_origin_joint_num][:2]\n",
        "    coords_[i,:,0] -= nose_x\n",
        "    coords_[i,:,1] -= nose_y\n",
        "\n",
        "  for i in range(len(coords_)):\n",
        "    coords[i][1][0] = coords_[i].transpose(1, 0)\n",
        "\n",
        "  pa = []\n",
        "  for i in range(len(coords_)):\n",
        "    pai = []\n",
        "    for j in range(len(coords_[i])):\n",
        "      pai.append(np.arctan2(coords_[i][j][1], coords_[i][j][0]))\n",
        "    pa.append(pai)\n",
        "\n",
        "  pa = np.array(pa)\n",
        "\n",
        "  vel_pa = np.diff(pa, axis = 0)\n",
        "\n",
        "  pv = []\n",
        "  coords[0][1][0][2] = 0\n",
        "  pv.append(coords[0][1][0][2])\n",
        "  for i in range(1, len(coords)):\n",
        "    coords[i][1][0][2] = vel_pa[i-1]\n",
        "    pv.append(vel_pa[i-1])\n",
        "\n",
        "  pv = np.array(pv)\n",
        "  acc = np.diff(pv, axis = 0)\n",
        "\n",
        "  coords[0][1][0][3] = 0\n",
        "  for i in range(1, len(coords)):\n",
        "    coords[i][1][0][3] = acc[i-1]\n",
        "\n",
        "  coords_ = []\n",
        "  for i in range(nfr):\n",
        "    coords_.append(coords[i][1][0].transpose(1, 0))\n",
        "\n",
        "  coords_ = np.array(coords_)\n",
        "\n",
        "  return tuple([coords, coords_, nfr, joint_lables])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcD3HEcphJ-8"
      },
      "source": [
        "<a name=\"storing-angle-data\"></a>\n",
        "## Store the velocity and acceleration in the 3rd and 4th component of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lZn4lGxS-Ov"
      },
      "source": [
        "file_names = []\n",
        "\n",
        "for gesture in gestures:\n",
        "    folder_path = \"/content/drive/MyDrive/CS256Project/data/gestures_visualized/\" + gesture\n",
        "    # print(folder_path)\n",
        "    for file in os.listdir(folder_path):\n",
        "        if os.path.isfile(os.path.join(folder_path, file)):\n",
        "            file_names.append(file)\n",
        "\n",
        "datapath = \"/content/drive/MyDrive/CS256Project/deduced_data\"\n",
        "if not os.path.exists(datapath): # checking if already exist\n",
        "    os.makedirs(datapath) # creating dir\n",
        "\n",
        "for file in file_names:\n",
        "    gesture = file.split('_',1)[0]\n",
        "    video_idx = file.split('_',1)[1].split('.')[0]\n",
        "    # print(gesture, video_idx)\n",
        "    _, coordst, n_frames, joint_lables = cal_vel_acc(gesture, video_idx)\n",
        "    datapath = f\"/content/drive/MyDrive/CS256Project/deduced_data/{gesture}\"\n",
        "    if not os.path.exists(datapath): # checking if already exist\n",
        "        os.makedirs(datapath) # creating dir\n",
        "    np.savez(f\"/content/drive/MyDrive/CS256Project/deduced_data/{gesture}/{file.split('.')[0]}\", arr = coordst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJfBeTOpS-Ov"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFIbMzY-R_OA"
      },
      "source": [
        "<a name=\"LSTM-classification\"></a>\n",
        "# Classification with LSTM (selected milestone 2 content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjy0hmuuSagu"
      },
      "source": [
        "<a name=\"Keras-data\"></a>\n",
        "## Data preparation for LSTM by Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdzl1nE6SsKb"
      },
      "source": [
        "# this functions creates folders to store test & train data\n",
        "\n",
        "def create_folders(subdirectories):\n",
        "    for subdirectory in subdirectories: # looping through folders\n",
        "        path = os.path.join(root, subdirectory)\n",
        "        if not os.path.exists(path): # checking if already exist\n",
        "            os.makedirs(path) # creating dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subdirectories = ['test', 'train', 'test/angle', 'test/velocity', 'test/acceleration', 'train/angle', 'train/velocity', 'train/acceleration'] + gestures\n",
        "create_folders(subdirectories)"
      ],
      "metadata": {
        "id": "jdkH9bjg1VR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnCamPSrSwx7"
      },
      "source": [
        "<a name=\"Train-test-split\"></a>\n",
        "## Split data to train(80%) and test(20%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0MxrOfkSwx8"
      },
      "source": [
        "# Define a file name structure\n",
        "train_file_x_name = '/train/{}/{}_gestures_x_train.txt'\n",
        "train_file_y_name = '/train/gestures_y_{}_train.txt'\n",
        "test_file_x_name = '/test/{}/{}_gestures_x_test.txt'\n",
        "test_file_y_name = '/test/gestures_y_{}_test.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gestures_2d_path = os.path.join(gestures_data_path, \"gestures_basic_d2\")\n",
        "MAX_FRAMES_PER_VIDEO = 72"
      ],
      "metadata": {
        "id": "AfhrgJ8b2OT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC_XJ2A5Swx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e40c6926-a5a5-486b-eab9-94517ae6eedc"
      },
      "source": [
        "# Get filenames for specific gesture\n",
        "def list_filenames_for_gesture(gesture):\n",
        "  # Define the path to the directory containing gesture files\n",
        "  file_path = f\"{gestures_2d_path}/{gesture}\"\n",
        "  # Get a list of filenames for gesture files in the directory\n",
        "  filenames = [fname for fname in os.listdir(file_path) if fname.endswith('.mp4.npz')]\n",
        "  return filenames\n",
        "print(list_filenames_for_gesture)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function list_filenames_for_gesture at 0x7ff2583c3250>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Then choose the relevant joints as features for the X_train\n",
        "relevant_joints = [5,6,7,8,9,10]\n",
        "joint_names = ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear', 'left_shoulder', 'Right_shoulder', 'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_hip', 'right_hip', 'left_knee', 'right_knee', 'left_ankle', 'right_ankle']"
      ],
      "metadata": {
        "id": "23BZFXsy1o1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get filenames for specific gesture\n",
        "def list_filenames_for_gesture(gesture):\n",
        "  # Define the path to the directory containing gesture files\n",
        "  file_path = f\"{gestures_2d_path}/{gesture}\"\n",
        "  # Get a list of filenames for gesture files in the directory\n",
        "  filenames = [fname for fname in os.listdir(file_path) if fname.endswith('.mp4.npz')]\n",
        "  return filenames\n",
        "print(list_filenames_for_gesture)"
      ],
      "metadata": {
        "id": "urkJFKj11qrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64db1f8e-9595-4357-ba30-c3ab6e8744b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function list_filenames_for_gesture at 0x7ff2583c32e0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def shift_coord(coords):\n",
        "    shifted_coords = []\n",
        "    for frame in coords:\n",
        "        current_frm_crds = frame[1][0].transpose(1, 0).copy() # get data in 2D form\n",
        "        nose_coordinate = current_frm_crds[0][:2].copy() # coordinates for nose joint\n",
        "        shifted_frame = []\n",
        "        for coord in current_frm_crds:\n",
        "            shifted_coord = [coord[0] - nose_coordinate[0], coord[1] - nose_coordinate[1], coord[2], coord[3]] # shifting coordinates\n",
        "            shifted_frame.append(shifted_coord) # storing in another variable\n",
        "        for i, coord in enumerate(shifted_frame):\n",
        "            angle = math.atan2(coord[1], coord[0]) # calculating polar angle\n",
        "            if i == 0:\n",
        "                vel = 0 # initial polar velocity is zero\n",
        "            else:\n",
        "                vel = coord[2] - shifted_frame[i-1][2] # calculating velocity\n",
        "            coord[2], coord[3] = angle, vel\n",
        "        shifted_coords.append(shifted_frame) # storing updated data2\n",
        "    return shifted_coords"
      ],
      "metadata": {
        "id": "bZObFsiq1scE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shifted_coords_gesture(gesture, file_name):\n",
        "    sample_2d_data_path = os.path.join(gestures_2d_path, gesture, file_name) # getting file path\n",
        "    data = np.load(sample_2d_data_path, allow_pickle=True)['keypoints'] # loading data for some gesture\n",
        "    return shift_coord(data)"
      ],
      "metadata": {
        "id": "ATPMeEhc26Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function shuffles the training and testing data for the given feature and writes the shuffled data to X and Y files\n",
        "def shuffle(feature, joints_train_data, joints_test_data, max_frames_in_any_video):\n",
        "    print(max_frames_in_any_video)\n",
        "    # Generate a random permutation of indexes for training and testing data\n",
        "    training_shuffled_indexes = np.random.permutation(len(joints_train_data[0]))\n",
        "    test_shuffled_indexes = np.random.permutation(len(joints_test_data[0]))\n",
        "\n",
        "    # Write shuffled training and testing data to X files\n",
        "    for joints_data, file_template in zip([joints_train_data, joints_test_data], [train_file_x_name, test_file_x_name]):\n",
        "        # Select the shuffled indexes based on whether the data is training or testing data\n",
        "        shuffled_indexes = training_shuffled_indexes if joints_data is joints_train_data else test_shuffled_indexes\n",
        "        # For each relevant joint, write the shuffled data to the corresponding X file\n",
        "        for rj_idx, joint_index in enumerate(relevant_joints):\n",
        "            joint_videos = joints_data[rj_idx]\n",
        "            #Pad data\n",
        "            joint_data_str = '\\n'.join([' '.join([str(x) for x in video_data] + ['0.0000000'] * (max_frames_in_any_video - len(video_data)))\n",
        "            for video_data in [joint_videos[i] for i in shuffled_indexes]])\n",
        "            file_x = file_template.format(feature, joint_names[joint_index])\n",
        "            # If the file already exists, remove it\n",
        "            if os.path.exists(root + file_x):\n",
        "                os.remove(root + file_x)\n",
        "            # Write the shuffled data to the X file\n",
        "            with open(root + file_x, 'a') as writeFile:\n",
        "                writeFile.write(joint_data_str)\n",
        "\n",
        "    # Write shuffled training and testing data to Y files\n",
        "    for joint_data, file_template in zip([joints_train_data, joints_test_data], [train_file_y_name, test_file_y_name]):\n",
        "        # Select the shuffled indexes based on whether the data is training or testing data\n",
        "        shuffled_indexes = training_shuffled_indexes if joint_data is joints_train_data else test_shuffled_indexes\n",
        "        # Get the target data for the joint\n",
        "        target_data = joint_data[-1]\n",
        "        file_y = file_template.format(feature)\n",
        "        # If the file already exists, remove it\n",
        "        if os.path.exists(root + file_y):\n",
        "            os.remove(root + file_y)\n",
        "        # Write the shuffled target data to the Y file\n",
        "        with open(root + file_y, 'a') as writeFile:\n",
        "            writeFile.write('\\n'.join([target_data[i] for i in shuffled_indexes]))\n"
      ],
      "metadata": {
        "id": "MNrxYFTj29mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(feature='velocity'):\n",
        "    # If the input feature is 'both', generate velocity data and then set the feature to 'angle'\n",
        "    if feature == 'both':\n",
        "        generate_data('velocity')\n",
        "        feature = 'angle'\n",
        "    # check if the input feature is in 'angle', 'velocity', or 'acceleration'\n",
        "    if feature not in ['angle', 'velocity', 'acceleration']:\n",
        "        raise ValueError('Wrong data argument', feature)\n",
        "\n",
        "    # Create empty lists to store training and testing data for relevant joints.\n",
        "    joints_train_data = [[] for _ in range(len(relevant_joints) + 1)]\n",
        "    joints_test_data = [[] for _ in range(len(relevant_joints) + 1)]\n",
        "    max_frames_in_any_video = 0\n",
        "\n",
        "    # Loop through each gesture in the list of gestures\n",
        "    for gesture_idx, gesture in enumerate(gestures):\n",
        "        # Get a list of file names for the current gesture\n",
        "        gesture_file_names = list_filenames_for_gesture(gesture)\n",
        "        # Select a random set of indices for test files\n",
        "        test_file_indices = set(random.sample(range(len(gesture_file_names)), math.floor(len(gesture_file_names) * 0.2)))\n",
        "\n",
        "        # Loop through each file name in the list of file names for the current gesture\n",
        "        for file_idx, file_name in enumerate(gesture_file_names):\n",
        "            # Get the shuffled coordinates for the current file name\n",
        "            shuffled_coords = np.array(shifted_coords_gesture(gesture, file_name))\n",
        "            no_of_frames = len(shuffled_coords)  # Get the number of frames in the shuffled coordinates\n",
        "\n",
        "            # If the number of frames in the shuffled coordinates is greater than the current maximum number of frames in any video, set it as the new maximum\n",
        "            if no_of_frames > max_frames_in_any_video:\n",
        "                max_frames_in_any_video = no_of_frames\n",
        "\n",
        "            # Determine whether to store the current data in the training or testing data list\n",
        "            joints_data = joints_test_data if file_idx in test_file_indices else joints_train_data\n",
        "\n",
        "            # Loop through each relevant joint index and append the Z-coordinates of the shuffled coordinates to the relevant joint data list\n",
        "            for rj_idx, joint_index in enumerate(relevant_joints):\n",
        "                joints_data[rj_idx].append(list(shuffled_coords[:, joint_index, 2]))\n",
        "\n",
        "            joints_data[-1].append(str(gesture_idx + 1)) # Append the gesture index plus one to the last element of the current joint data list\n",
        "        print(gesture, max_frames_in_any_video)\n",
        "        print(\"----------------------\")\n",
        "    # shuffle the training and testing data for given feature and the maximum number of frames in any video\n",
        "    shuffle(feature, joints_train_data, joints_test_data, 144)"
      ],
      "metadata": {
        "id": "XzFbNfdr3AAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_data('both') # DO NOT RUN AGAIN"
      ],
      "metadata": {
        "id": "dbKMRYI33E74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e6c1cc9-3089-4085-a307-dc3c338855e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "circle 72\n",
            "----------------------\n",
            "turn 72\n",
            "----------------------\n",
            "144\n",
            "circle 72\n",
            "----------------------\n",
            "turn 72\n",
            "----------------------\n",
            "144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def re_shift_coord_acc(coords):\n",
        "    shifted_coords = shift_coord(coords) # getting original shifted coordinate data\n",
        "    for frame in shifted_coords:\n",
        "        for coord in frame:\n",
        "            coord[2] = coord[3] # putting velocity in 3rd column\n",
        "\n",
        "        for i, coord in enumerate(frame):\n",
        "            if i == 0:\n",
        "                acc = 0 # initial acceleration is zero\n",
        "            else:\n",
        "                acc = coord[2] - frame[i-1][2] # calculating acceleration as change in velocity\n",
        "            coord[3] = acc\n",
        "\n",
        "    return shifted_coords # returning updated data"
      ],
      "metadata": {
        "id": "vw5DtyhN1v0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shifted_coords_gesture(gesture, file_name): # redefining shifted_coords_gesture function to work with acceleration\n",
        "    sample_2d_data_path = os.path.join(gestures_2d_path, gesture, file_name) # getting file path\n",
        "    data = np.load(sample_2d_data_path, allow_pickle=True)['keypoints'] # loading data for some gesture\n",
        "    return re_shift_coord_acc(data)"
      ],
      "metadata": {
        "id": "7sh4egCQ16P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_data('acceleration')"
      ],
      "metadata": {
        "id": "pQs78gED2DEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb256302-5c58-47b1-96fd-bc106decd999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "circle 72\n",
            "----------------------\n",
            "turn 72\n",
            "----------------------\n",
            "144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading Data"
      ],
      "metadata": {
        "id": "k5ErqS941XrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "\treturn dataframe.values"
      ],
      "metadata": {
        "id": "zyW9YnP0cq4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "\tloaded = list()\n",
        "\tfor name in filenames:\n",
        "\t\tdata = load_file(prefix + name)\n",
        "\t\tloaded.append(data)\n",
        "\t# stack group so that features are the 3rd dimension\n",
        "\tloaded = dstack(loaded)\n",
        "\treturn loaded"
      ],
      "metadata": {
        "id": "1yVGQZWIcs--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add files to the path\n",
        "def add_filenames(filenames, data, group):\n",
        "  for joint_index in relevant_joints:\n",
        "\t  filenames += ['/{}/{}_gestures_x_{}.txt'.format(data, joint_names[joint_index], group)]"
      ],
      "metadata": {
        "id": "45f6I9dWcuOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(data, group, prefix=''):\n",
        "\tfilepath = prefix + '/' + group\n",
        "\tfilenames = list()\n",
        "\tif data == 'both':\n",
        "\t\tadd_filenames(filenames, 'angle', group)\n",
        "\t\tadd_filenames(filenames, 'velocity', group)\n",
        "\telse:\n",
        "\t\tadd_filenames(filenames, data, group)\n",
        "\n",
        "\tprint('filenames:', filenames)\n",
        "\t# print('filenames shape:', filenames.shape)\n",
        "\n",
        "\t# load input data\n",
        "\tX = load_group(filenames, filepath)\n",
        "\t# load class output\n",
        "\ty = load_file(prefix + '/' + group + '/gestures_y_' + data + '_' +group+'.txt')\n",
        "\treturn X, y"
      ],
      "metadata": {
        "id": "Q5y7Sa79cvWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(data, prefix=''):\n",
        "\t# load all train\n",
        "\ttrainX, trainy = load_dataset_group(data, 'train', prefix)\n",
        "\tprint(trainX.shape, trainy.shape)\n",
        "\t# load all test\n",
        "\ttestX, testy = load_dataset_group(data, 'test', prefix)\n",
        "\tprint(testX.shape, testy.shape)\n",
        "\t# zero-offset class values\n",
        "\ttrainy = trainy - 1\n",
        "\ttesty = testy - 1\n",
        "\t# one hot encode y\n",
        "\ttrainy = to_categorical(trainy)\n",
        "\ttesty = to_categorical(testy)\n",
        "\tprint('[samples, time steps, features]',trainX.shape, trainy.shape, '[samples, time steps, features]',testX.shape, testy.shape)\n",
        "\treturn trainX, trainy, testX, testy"
      ],
      "metadata": {
        "id": "VZ3XjdS-cw1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2eRlI-xUK3X"
      },
      "source": [
        "<a name=\"lstm-model\"></a>\n",
        "## LSTM model performance with different configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYaxy3ECUK3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7436dfd-11ac-4d4f-dfe3-aee72d3b8480"
      },
      "source": [
        "trainX, trainy, testX, testy = load_dataset('acceleration', '/content/drive/MyDrive/CS256Project/data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filenames: ['/acceleration/left_shoulder_gestures_x_train.txt', '/acceleration/Right_shoulder_gestures_x_train.txt', '/acceleration/left_elbow_gestures_x_train.txt', '/acceleration/right_elbow_gestures_x_train.txt', '/acceleration/left_wrist_gestures_x_train.txt', '/acceleration/right_wrist_gestures_x_train.txt']\n",
            "(99, 144, 6) (99, 1)\n",
            "filenames: ['/acceleration/left_shoulder_gestures_x_test.txt', '/acceleration/Right_shoulder_gestures_x_test.txt', '/acceleration/left_elbow_gestures_x_test.txt', '/acceleration/right_elbow_gestures_x_test.txt', '/acceleration/left_wrist_gestures_x_test.txt', '/acceleration/right_wrist_gestures_x_test.txt']\n",
            "(24, 144, 6) (24, 1)\n",
            "[samples, time steps, features] (99, 144, 6) (99, 2) [samples, time steps, features] (24, 144, 6) (24, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo9-JAdiUK3Y"
      },
      "source": [
        "import itertools\n",
        "# params_grid = {'num_lstm_layers':[1,2,3],\n",
        "#                'lstm_neurons':[128, 150, 200],\n",
        "#                'dropout_rate':[0.3, 0.4, 0.5],\n",
        "#                'num_dense_layers':[1,2,3,4],\n",
        "#                'dense_neurons':[64, 128],\n",
        "#                'batch_size':[32, 64]}\n",
        "\n",
        "params_grid = {'num_lstm_layers':[3],\n",
        "               'lstm_neurons':[150],\n",
        "               'dropout_rate':[0.5],\n",
        "               'num_dense_layers':[2],\n",
        "               'dense_neurons':[128],\n",
        "               'batch_size':[64]}\n",
        "\n",
        "\n",
        "param_keys = params_grid.keys()\n",
        "param_values = params_grid.values()\n",
        "\n",
        "# Generate all possible permutations\n",
        "param_permutations = list(itertools.product(*param_values))\n",
        "\n",
        "# Create a list of dictionaries for each permutation\n",
        "param_configs = [dict(zip(param_keys, param_perm)) for param_perm in param_permutations]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes in the following arguments:\n",
        "# trainX: training input data\n",
        "# trainy: training output/target data\n",
        "# testX: testing input data\n",
        "# testy: testing output/target data\n",
        "# num_lstm_layers: number of LSTM layers\n",
        "# lstm_neurons: number of LSTM neurons\n",
        "# dropout_rate: rate of dropout for regularization\n",
        "# num_dense_layers: number of dense layers\n",
        "# dense_neurons: number of neurons in each dense layer\n",
        "# batch_size: size of mini-batch for training\n",
        "\n",
        "def evaluate_model(trainX, trainy, testX, testy, num_lstm_layers, lstm_neurons, dropout_rate, num_dense_layers, dense_neurons, batch_size):\n",
        "\tverbose, epochs = 0, 15 # Set verbose and number of epochs\n",
        "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1] # Get the number of timesteps, features, and outputs\n",
        "\tmodel = Sequential() # Create a Sequential model\n",
        "\tfor i in range(1, num_lstm_layers): # Add specified number of LSTM layers with specified number of neurons and input shape\n",
        "\t\tmodel.add(LSTM(lstm_neurons, input_shape=(n_timesteps,n_features),return_sequences=True))\n",
        "\tmodel.add(LSTM(lstm_neurons, input_shape=(n_timesteps,n_features)))\n",
        "\tmodel.add(Dropout(dropout_rate)) # Add a dropout layer with specified rate\n",
        "\tfor i in range(num_dense_layers): # Add the specified number of dense layers with specified number of neurons and ReLU activation\n",
        "\t\tmodel.add(Dense(dense_neurons, activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs, activation='softmax')) # Add a dense output layer with softmax activation\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Compile the model with categorical crossentropy loss, Adam optimizer, and accuracy metric\n",
        "\n",
        "\tmodel.summary() # Print model summary\n",
        "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose) # Fit the model to the training data\n",
        "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0) # Evaluate the model on the testing data and get the accuracy\n",
        "\n",
        "\t# return accuracy # Return the accuracy\n",
        "\treturn accuracy\n"
      ],
      "metadata": {
        "id": "jAlwymzN39vM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize scores\n",
        "def summarize_results(scores, params, data_point):\n",
        "\t# print(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
        "\tprint(f'Max Accuracy of {max(scores)} obtained with {params[scores.index(max(scores))]}')\n",
        "\tscores_df = pd.DataFrame.from_dict(params)\n",
        "\tscores_df['Accuracy'] = scores\n",
        "\tscores_df.to_csv(f\"{root}/model_architectures_{data_point}.csv\", sep=\",\", index=False)"
      ],
      "metadata": {
        "id": "2J7WN4EH3__P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to run an experiment\n",
        "def run_experiment(data_point):\n",
        "\t# load data\n",
        "\tglobal trainX, trainy, testX, testy\n",
        "\t# repeat experiment\n",
        "\tscores = list()\n",
        "\tfor params in param_configs: # looping through each configuration\n",
        "\t\tscore = evaluate_model(trainX, trainy, testX, testy, params['num_lstm_layers'], params['lstm_neurons'], params['dropout_rate'],\n",
        "                         params['num_dense_layers'], params['dense_neurons'], params['batch_size'])\t  # evaluating model performance for specific parameters\n",
        "\t\tscore = score * 100.0 # getting score as a percentage\n",
        "\t\tprint(f'>Score with {params}: {format(score, \".3f\")}')  # printing accuracy score\n",
        "\t\tscores.append(score)\n",
        "\t# summarize results\n",
        "\tsummarize_results(scores, param_configs, data_point)"
      ],
      "metadata": {
        "id": "TqhBvy3K4EXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# run the experiment\n",
        "run_experiment(\"acceleration\")"
      ],
      "metadata": {
        "id": "HbTaLWUG4E-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e02d0951-572f-4184-c508-f1ac4371841e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 144, 150)          94200     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 144, 150)          180600    \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 150)               180600    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 150)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               19328     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 491,498\n",
            "Trainable params: 491,498\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1Snf2OzUwaZ"
      },
      "source": [
        "<a name=\"differnt-datasets\"></a>\n",
        "## LSTM model performance with different datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS11TJouUwaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99124227-62ab-4e59-90bf-1b89bd70bfd3"
      },
      "source": [
        "trainXl, trainyl, testXl, testyl = {}, {}, {}, {} # dictionaries to store train & test data\n",
        "for data_points in ['angle', 'velocity', 'acceleration']: # retriving train & test data for each data point\n",
        "  trainXl[data_points], trainyl[data_points], testXl[data_points], testyl[data_points] = load_dataset(data_points, '/content/drive/MyDrive/CS256Project/data/data_for_lstm')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filenames: ['/angle/left_shoulder_gestures_x_train.txt', '/angle/Right_shoulder_gestures_x_train.txt', '/angle/left_elbow_gestures_x_train.txt', '/angle/right_elbow_gestures_x_train.txt', '/angle/left_wrist_gestures_x_train.txt', '/angle/right_wrist_gestures_x_train.txt']\n",
            "(99, 72, 6) (99, 1)\n",
            "filenames: ['/angle/left_shoulder_gestures_x_test.txt', '/angle/Right_shoulder_gestures_x_test.txt', '/angle/left_elbow_gestures_x_test.txt', '/angle/right_elbow_gestures_x_test.txt', '/angle/left_wrist_gestures_x_test.txt', '/angle/right_wrist_gestures_x_test.txt']\n",
            "(24, 72, 6) (24, 1)\n",
            "[samples, time steps, features] (99, 72, 6) (99, 2) [samples, time steps, features] (24, 72, 6) (24, 2)\n",
            "filenames: ['/velocity/left_shoulder_gestures_x_train.txt', '/velocity/Right_shoulder_gestures_x_train.txt', '/velocity/left_elbow_gestures_x_train.txt', '/velocity/right_elbow_gestures_x_train.txt', '/velocity/left_wrist_gestures_x_train.txt', '/velocity/right_wrist_gestures_x_train.txt']\n",
            "(99, 72, 6) (99, 1)\n",
            "filenames: ['/velocity/left_shoulder_gestures_x_test.txt', '/velocity/Right_shoulder_gestures_x_test.txt', '/velocity/left_elbow_gestures_x_test.txt', '/velocity/right_elbow_gestures_x_test.txt', '/velocity/left_wrist_gestures_x_test.txt', '/velocity/right_wrist_gestures_x_test.txt']\n",
            "(24, 72, 6) (24, 1)\n",
            "[samples, time steps, features] (99, 72, 6) (99, 2) [samples, time steps, features] (24, 72, 6) (24, 2)\n",
            "filenames: ['/acceleration/left_shoulder_gestures_x_train.txt', '/acceleration/Right_shoulder_gestures_x_train.txt', '/acceleration/left_elbow_gestures_x_train.txt', '/acceleration/right_elbow_gestures_x_train.txt', '/acceleration/left_wrist_gestures_x_train.txt', '/acceleration/right_wrist_gestures_x_train.txt']\n",
            "(99, 72, 6) (99, 1)\n",
            "filenames: ['/acceleration/left_shoulder_gestures_x_test.txt', '/acceleration/Right_shoulder_gestures_x_test.txt', '/acceleration/left_elbow_gestures_x_test.txt', '/acceleration/right_elbow_gestures_x_test.txt', '/acceleration/left_wrist_gestures_x_test.txt', '/acceleration/right_wrist_gestures_x_test.txt']\n",
            "(24, 72, 6) (24, 1)\n",
            "[samples, time steps, features] (99, 72, 6) (99, 2) [samples, time steps, features] (24, 72, 6) (24, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nraCAvr0Uwaa"
      },
      "source": [
        "# This function takes in the following arguments:\n",
        "# trainX: training input data\n",
        "# trainy: training output/target data\n",
        "# testX: testing input data\n",
        "# testy: testing output/target data\n",
        "# lstm_neurons: number of LSTM neurons\n",
        "# dropout_rate: rate of dropout for regularization\n",
        "# num_dense_layers: number of dense layers\n",
        "# dense_neurons: number of neurons in each dense layer\n",
        "# batch_size: size of mini-batch for training\n",
        "\n",
        "def evaluate_model_compare(trainX, trainy, testX, testy, lstm_neurons, dropout_rate, num_dense_layers, dense_neurons, batch_size):\n",
        "    verbose, epochs = 0, 15 # Set verbose and number of epochs\n",
        "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1] # Get the number of timesteps, features, and outputs\n",
        "    model = Sequential() # Create a Sequential model\n",
        "    model.add(LSTM(lstm_neurons, input_shape=(n_timesteps,n_features)))  # Add an LSTM layer with specified number of neurons and input shape\n",
        "\n",
        "    for i in range(num_dense_layers):\n",
        "        model.add(Dense(dense_neurons, activation='relu'))  # Add the specified number of dense layers with specified number of neurons and ReLU activation\n",
        "\n",
        "    model.add(Dense(n_outputs, activation='softmax')) # Add a dense output layer with softmax activation\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Compile the model with categorical crossentropy loss, Adam optimizer, and accuracy metric\n",
        "\n",
        "    # Fit the model to the training data\n",
        "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "    # Evaluate the model on the testing data and get the accuracy\n",
        "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    return accuracy # Return the accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "# params_grid = {'lstm_neurons':[128, 150],\n",
        "#                'dropout_rate':[0.4, 0.5],\n",
        "#                'num_dense_layers':[1, 2],\n",
        "#                'dense_neurons':[64, 128],\n",
        "#                'batch_size':[32, 64]}\n",
        "\n",
        "params_grid = {'lstm_neurons':[128],\n",
        "               'dropout_rate':[0.5],\n",
        "               'num_dense_layers':[1],\n",
        "               'dense_neurons':[64],\n",
        "               'batch_size':[64]}\n",
        "\n",
        "\n",
        "param_configs = [dict(zip(params_grid.keys(), param_perm)) for param_perm in list(itertools.product(*params_grid.values()))]"
      ],
      "metadata": {
        "id": "FFX02ZpL4UbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_compare_experiment(param_config):\n",
        " global trainXl, trainyl, testXl, testyl\n",
        "\n",
        " for params in param_config:  # looping through each configuration\n",
        "  print(f'\\n||Params: {params}')\n",
        "  for data_point in ['angle', 'velocity', 'acceleration']: # checking for each feature\n",
        "    score = evaluate_model_compare(trainXl[data_point], trainyl[data_point], testXl[data_point], testyl[data_point], params['lstm_neurons'], params['dropout_rate'],\n",
        "              params['num_dense_layers'], params['dense_neurons'], params['batch_size'])  # evaluating model performance for specific parameters\n",
        "    score = score * 100.0 # getting score as a percentage\n",
        "    print(f'  >{data_point}: {format(score, \".3f\")}')  # printing accuracy score\n"
      ],
      "metadata": {
        "id": "vE7rcU4D4WUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "run_compare_experiment(param_configs)"
      ],
      "metadata": {
        "id": "Gh6bjUpg4YYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "## Finding best feature and parameters by manually checking for varying parameters\n",
        "\n",
        "param_configs = [\n",
        "{'lstm_neurons': 128, 'dropout_rate': 0.4, 'num_dense_layers': 1, 'dense_neurons': 64, 'batch_size': 32},\n",
        "{'lstm_neurons': 128, 'dropout_rate': 0.4, 'num_dense_layers': 2, 'dense_neurons': 64, 'batch_size': 32},\n",
        "{'lstm_neurons': 128, 'dropout_rate': 0.5, 'num_dense_layers': 1, 'dense_neurons': 64, 'batch_size': 32},\n",
        "{'lstm_neurons': 128, 'dropout_rate': 0.5, 'num_dense_layers': 1, 'dense_neurons': 128, 'batch_size': 32},\n",
        "{'lstm_neurons': 150, 'dropout_rate': 0.4, 'num_dense_layers': 1, 'dense_neurons': 64, 'batch_size': 64},\n",
        "{'lstm_neurons': 150, 'dropout_rate': 0.4, 'num_dense_layers': 2, 'dense_neurons': 128, 'batch_size': 64},\n",
        "{'lstm_neurons': 150, 'dropout_rate': 0.5, 'num_dense_layers': 1, 'dense_neurons': 128, 'batch_size': 64}]\n",
        "\n",
        "run_compare_experiment(param_configs)"
      ],
      "metadata": {
        "id": "_9wJhz5A4oRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GieO_biXVdwB"
      },
      "source": [
        "<a name=\"performance-reasons\"></a>\n",
        "## Reasons for the varying performance due to different model configurations or datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ARnJkaPVdwC"
      },
      "source": [
        "#Print average scores for varying parameters on each set of data\n",
        "angle_scores = [50.000, 79.167, 62.500, 66.667, 91.667, 79.167, 75.000]\n",
        "velocity_scores = [83.333, 66.667,66.667, 79.167, 75.000,  62.500, 54.167]\n",
        "acceleration_scores = [91.667, 87.500, 91.667,  75.000,91.667, 79.167,83.333]\n",
        "\n",
        "print(\"Mean Angle Score: \" + str(np.mean(angle_scores)) )\n",
        "print(\"Mean Velocity Score: \" + str(mean(velocity_scores)))\n",
        "print(\"Mean Acceleration Score: \" + str(mean(acceleration_scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Acceleration is found to be the best feature**\n",
        "\n",
        "*   Consistently gave higher accuracy\n",
        "*   Does no overfitting or underfitting\n",
        "*   Models consumed relatively less CPU time"
      ],
      "metadata": {
        "id": "rmMU5PA8422h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgAaUm9kV9iA"
      },
      "source": [
        "<a name=\"optimal-model\"></a>\n",
        "# Analyzing an optimal classification model(milestone 3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gestures = ['circle', 'turn', 'warn']"
      ],
      "metadata": {
        "id": "reJ8yRFMk4gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_folders(gestures)"
      ],
      "metadata": {
        "id": "HJWpf3w8l3jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul5Z5Pp55Q-J"
      },
      "source": [
        "<a name=\"concatevideos\"></a>\n",
        "## Creation of Concatenated videos with two gestures in a sequence\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_padded_data(feature='acceleration'):\n",
        "  max_frames_in_any_video = 0\n",
        "\n",
        "  # Loop through each gesture in the list of gestures\n",
        "  for gesture_idx, gesture in enumerate(gestures):\n",
        "    # Get a list of file names for the current gesture\n",
        "    gesture_file_names = list_filenames_for_gesture(gesture)\n",
        "    coords_list = []\n",
        "  # Loop through each file name in the list of file names for the current gesture\n",
        "    for file_idx, file_name in enumerate(gesture_file_names):\n",
        "      # Get the shuffled coordinates for the current file name\n",
        "      shuffled_coords = np.array(shifted_coords_gesture(gesture, file_name))\n",
        "      coords_list.append(shuffled_coords)\n",
        "\n",
        "    #Pad the coords list\n",
        "    final_coords_list = pad_sequences(coords_list, padding='post', value=0, dtype='float', maxlen = MAX_FRAMES_PER_VIDEO)\n",
        "    # print(len(final_coords_list))\n",
        "    # for i in range(len(final_coords_list)):\n",
        "    for i in range(len(final_coords_list)):\n",
        "      np.savez(f\"{root}/{gesture}/{gesture_file_names[i]}\", arr=final_coords_list[i])"
      ],
      "metadata": {
        "id": "Xk6oNEbU5V2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_padded_data() # Do Not Run Again"
      ],
      "metadata": {
        "id": "3w17-sDw6M0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "circle_paths, turn_paths, warn_paths = [], [], []"
      ],
      "metadata": {
        "id": "xckh652I7h1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_full_paths(directory):\n",
        "    return [os.path.join(directory, file) for file in os.listdir(directory)]"
      ],
      "metadata": {
        "id": "BJwJTj8J7kwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the paths of all videos in each gesture folder\n",
        "\n",
        "for gesture in gestures:\n",
        "    if gesture == \"warn\" and os.path.exists(os.path.join(root, gesture)):\n",
        "        warn_paths.extend(list_full_paths(os.path.join(root, gesture)))\n",
        "    elif gesture == \"circle\" and os.path.exists(os.path.join(root, gesture)):\n",
        "        circle_paths.extend(list_full_paths(os.path.join(root, gesture)))\n",
        "    elif gesture == \"turn\" and os.path.exists(os.path.join(root, gesture)):\n",
        "        turn_paths.extend(list_full_paths(os.path.join(root, gesture)))"
      ],
      "metadata": {
        "id": "oUqwWXwQ7mnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create directories for each pair of concatenated gestures\n",
        "os.makedirs(\"/content/drive/MyDrive/CS256Project/Milestone3/combined_data_base\")\n",
        "for g1 in gestures:\n",
        "    for g2 in gestures:\n",
        "        if g1==g2:\n",
        "            continue\n",
        "        else:\n",
        "            path = os.path.join(\"/content/drive/MyDrive/CS256Project/Milestone3/combined_data_base\", f\"{g1}_{g2}\")\n",
        "            if not os.path.exists(path): # checking if already exist\n",
        "                os.makedirs(path) # creating dir"
      ],
      "metadata": {
        "id": "OuGl9B947wPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN AGAIN\n",
        "# AS IT SAVES ALL THE CONCATENATED .NPZ ARRAYS IN DRIVE\n",
        "#Concatenate and save all npz files in their respective subdirectories\n",
        "i = 1\n",
        "for path1 in circle_paths:\n",
        "    video1 = np.load(path1, allow_pickle=True)['arr']\n",
        "    j,k = 1,1\n",
        "    for path2 in turn_paths:\n",
        "        video2 = np.load(path2, allow_pickle=True)['arr']\n",
        "        np.savez(f\"/content/drive/MyDrive/CS256Project/Milestone3/combined_data_base/circle_turn/circle_turn_{i}_{j}\", arr = np.array([video1, video2]))\n",
        "        np.savez(f\"/content/drive/MyDrive/CS256Project/Milestone3/combined_data_base/turn_circle/turn_circle_{i}_{j}\", arr = np.array([video2, video1]))\n",
        "        j+=1\n",
        "\n",
        "    for path3 in warn_paths:\n",
        "        video3 = np.load(path3, allow_pickle=True)['arr']\n",
        "        np.savez(f\"/content/drive/MyDrive/CS256Project/Milestone3/combined_data_base/circle_warn/circle_warn_{i}_{k}\", arr = np.array([video1, video3]))\n",
        "        np.savez(f\"/content/drive/MyDrive/CS256Project/Milestone3/combined_data_base/warn_circle/warn_circle_{i}_{k}\", arr = np.array([video3, video1]))\n",
        "        k+=1\n",
        "    i+=1\n",
        "\n",
        "\n",
        "i = 1\n",
        "for path1 in turn_paths:\n",
        "    video1 = np.load(path1, allow_pickle=True)['arr']\n",
        "    j = 1\n",
        "    for path2 in warn_paths:\n",
        "        video2 = np.load(path2, allow_pickle=True)['arr']\n",
        "        np.savez(f\"/content/drive/MyDrive/CS256Project/Milestone3/combined_data_base/turn_warn/turn_warn{i}_{j}\", arr = np.array([video1, video2]))\n",
        "        np.savez(f\"/content/drive/MyDrive/CS256Project/Milestone3/combined_data_base/warn_turn/warn_turn{i}_{j}\", arr = np.array([video2, video1]))\n",
        "        j+=1\n",
        "    i+=1\n"
      ],
      "metadata": {
        "id": "sVb9dOvH72e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir(\"/content/drive/MyDrive/CS256Project/Milestone3/combined_data_base/circle_turn/\"))>0)"
      ],
      "metadata": {
        "id": "H-DJnN8O75h7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "512ae584-7f0e-46fd-b746-bb9db3cc1220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_labels = []\n",
        "target_labels = []\n",
        "for g1 in gestures:\n",
        "    for g2 in gestures:\n",
        "        if g1 != g2:\n",
        "            combined_labels.append(f\"{g1}_{g2}\")\n",
        "            target_labels.append([gestures.index(g1), gestures.index(g2)])"
      ],
      "metadata": {
        "id": "Q01-paE58nwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_labels"
      ],
      "metadata": {
        "id": "0VGYJ3YI7ZpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee7b3ac2-0d2f-4025-9a5b-d1d6dd25560a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['circle_turn',\n",
              " 'circle_warn',\n",
              " 'turn_circle',\n",
              " 'turn_warn',\n",
              " 'warn_circle',\n",
              " 'warn_turn']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare X_train, X_test, y_train and y_test for concatenated videos"
      ],
      "metadata": {
        "id": "3H41j2xy_GQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Fetch only the required joints from the keypoints in the data\n",
        "def get_data(class_label):\n",
        "  data = []\n",
        "  data_path = os.path.join(\"/content/drive/MyDrive/CS256Project/Milestone3/combined_data_base\", class_label)\n",
        "  for filename in os.listdir(data_path):\n",
        "    # get path of each file\n",
        "    file_path = os.path.join(data_path, filename)\n",
        "    # print(file_path)\n",
        "    # load the numpy array from the input file\n",
        "    l = np.load(file_path, allow_pickle=True)\n",
        "    sample = []\n",
        "    for i, frame in enumerate(l['arr']):\n",
        "      #extract only the relevant joints (shoulders, elbows and wrists - both left and right)\n",
        "      sample.append(frame)\n",
        "    data.append(np.array(sample))\n",
        "  return np.array(data)"
      ],
      "metadata": {
        "id": "cBbsQhRxOzyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "Y = []\n",
        "combined_data_path = \"/content/drive/MyDrive/CS256Project/Milestone3/combined_data_base/\"\n",
        "for idx in range(len(combined_labels)):\n",
        "    X.extend(get_data(combined_labels[idx]))\n",
        "    Y.extend([target_labels[idx]] * len(os.listdir(os.path.join(combined_data_path, combined_labels[idx]))))"
      ],
      "metadata": {
        "id": "k6968wQPPAvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function used to extract polar angles, polar velocity and polar acceleration separately.\n",
        "def extract_data(data, index):\n",
        "  res = []\n",
        "  for sample in data:\n",
        "    new_sample = []\n",
        "    for video in sample:\n",
        "        new_video=[]\n",
        "        for frame in video:\n",
        "            polar_angles = []\n",
        "            for joint in frame[5:11]:\n",
        "                polar_angles.append(joint[index])\n",
        "            new_video.append(polar_angles)\n",
        "        new_sample.append(new_video)\n",
        "    res.append(new_sample)\n",
        "  return np.array(res)\n"
      ],
      "metadata": {
        "id": "1zF33KwXO0He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling extract_data() to fetch only polar acceleration for testing with LSTM.\n",
        "X_acceleration = extract_data(np.array(X), 3)\n",
        "# X_test = extract_data(X_test_all, 2)"
      ],
      "metadata": {
        "id": "F-rj0zgRP-09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_acceleration.shape)\n",
        "print(np.array(Y).shape)"
      ],
      "metadata": {
        "id": "-cK-KnWUwLSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_acceleration, np.array(Y), test_size = 0.2, random_state=30)"
      ],
      "metadata": {
        "id": "6_W-CKPxQBdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testpath = '/content/drive/MyDrive/CS256Project/Milestone3/Combined_video_train_test'"
      ],
      "metadata": {
        "id": "W9oLIhs1RXrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(testpath): # checking if already exist\n",
        "    os.makedirs(testpath)\n",
        "np.savez(f'{testpath}/X_train.npz', arr=X_train)\n",
        "np.savez(f'{testpath}/X_test.npz', arr=X_test)\n",
        "np.savez(f'{testpath}/y_train.npz', arr=y_train)\n",
        "np.savez(f'{testpath}/y_test.npz', arr=y_test)"
      ],
      "metadata": {
        "id": "t1u6w_ZoQ6l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5hs6SyvTOFQ"
      },
      "source": [
        "<a name=\"GridSearchCV\"></a>\n",
        "## Model Evaluation\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Approach 1 : Use the model trained on single single gesture and predict a sequence using custom prediction function"
      ],
      "metadata": {
        "id": "F1_2BysN75jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Fetch only the required joints from the keypoints in the data\n",
        "def get_data(class_label):\n",
        "  data = []\n",
        "  data_path = os.path.join(\"/content/drive/MyDrive/CS256Project/data\", class_label)\n",
        "  for filename in os.listdir(data_path):\n",
        "    #get path of each file\n",
        "    file_path = os.path.join(data_path, filename)\n",
        "    # print(file_path)\n",
        "    #load the numpy array from the input file\n",
        "    l = np.load(file_path, allow_pickle=True)\n",
        "    sample = []\n",
        "    for i, frame in enumerate(l['arr']):\n",
        "      #extract only the relevant joints (shoulders, elbows and wrists - both left and right)\n",
        "      sample.append(frame[5:11])\n",
        "    data.append(np.array(sample))\n",
        "  return np.array(data)"
      ],
      "metadata": {
        "id": "gTYFqQgmCaL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "circle_samples = get_data('circle')\n",
        "turn_sample = get_data('turn')\n",
        "warn_sample = get_data('warn')"
      ],
      "metadata": {
        "id": "GmIXpHlTCdfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combining all gestures and padding the data\n",
        "X_values = np.vstack((circle_samples, turn_sample, warn_sample))\n",
        "#target value has 2 variables, isHello and isAbort. Target values for hello sample = [1,0] and target values for abort sample = [0,1]\n",
        "target_values = np.array([[1, 0, 0]] * len(circle_samples)\n",
        "                + [[0, 1, 0]] * len(turn_sample)\n",
        "                + [[0, 0, 1]] * len(warn_sample))"
      ],
      "metadata": {
        "id": "fwjqLHJOCgxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_values.shape)\n",
        "print(target_values.shape)"
      ],
      "metadata": {
        "id": "8BexPHPNCkEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function used to extract polar angles, polar velocity and polar acceleration separately.\n",
        "def extract_data(data, index):\n",
        "  res = []\n",
        "  for sample in data:\n",
        "    new_sample = []\n",
        "    for frame in sample:\n",
        "      polar_angles = []\n",
        "      for joint in frame:\n",
        "        polar_angles.append(joint[index])\n",
        "      new_sample.append(polar_angles)\n",
        "    res.append(new_sample)\n",
        "  return np.array(res)"
      ],
      "metadata": {
        "id": "E-TCy5_nCmK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling extract_data() to fetch only polar acceleration for testing with LSTM.\n",
        "X_acceleration = extract_data(X_values, 3)"
      ],
      "metadata": {
        "id": "MRRm3CSrCoe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#split the data into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_acceleration, target_values, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "u8S1IJheCqym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "_TeW5lGz87HA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model Training"
      ],
      "metadata": {
        "id": "CV5tcl0J9YKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes in the following arguments:\n",
        "# trainX: training input data\n",
        "# trainy: training output/target data\n",
        "# testX: testing input data\n",
        "# testy: testing output/target data\n",
        "# num_lstm_layers: number of LSTM layers\n",
        "# lstm_neurons: number of LSTM neurons\n",
        "# dropout_rate: rate of dropout for regularization\n",
        "# num_dense_layers: number of dense layers\n",
        "# dense_neurons: number of neurons in each dense layer\n",
        "# batch_size: size of mini-batch for training\n",
        "\n",
        "def evaluate_model(trainX, trainy, testX, testy, num_lstm_layers, lstm_neurons, dropout_rate, num_dense_layers, dense_neurons, batch_size):\n",
        "\tverbose, epochs = 0, 15 # Set verbose and number of epochs\n",
        "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1] # Get the number of timesteps, features, and outputs\n",
        "\tmodel = Sequential() # Create a Sequential model\n",
        "\tfor i in range(1, num_lstm_layers): # Add specified number of LSTM layers with specified number of neurons and input shape\n",
        "\t\tmodel.add(LSTM(lstm_neurons, input_shape=(n_timesteps,n_features),return_sequences=True))\n",
        "\t\tmodel.add(Dropout(dropout_rate)) # Add a dropout layer with specified rate\n",
        "\tmodel.add(LSTM(lstm_neurons, input_shape=(n_timesteps,n_features)))\n",
        "\tmodel.add(Dropout(dropout_rate)) # Add a dropout layer with specified rate\n",
        "\tfor i in range(num_dense_layers): # Add the specified number of dense layers with specified number of neurons and ReLU activation\n",
        "\t\tmodel.add(Dense(dense_neurons, activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs, activation='softmax')) # Add a dense output layer with softmax activation\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Compile the model with categorical crossentropy loss, Adam optimizer, and accuracy metric\n",
        "\n",
        "\tmodel.summary() # Print model summary\n",
        "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose) # Fit the model to the training data\n",
        "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0) # Evaluate the model on the testing data and get the accuracy\n",
        "\n",
        "\t# return accuracy # Return the accuracy\n",
        "\treturn model, accuracy"
      ],
      "metadata": {
        "id": "B8f3OvCF9Z-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize scores\n",
        "def summarize_results(scores, params, data_point):\n",
        "\t# print(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
        "\tprint(f'Max Accuracy of {max(scores)} obtained with {params[scores.index(max(scores))]}')\n",
        "\tscores_df = pd.DataFrame.from_dict(params)\n",
        "\tscores_df['Accuracy'] = scores\n",
        "\tscores_df.to_csv(f\"{root}/model_architectures_{data_point}.csv\", sep=\",\", index=False)"
      ],
      "metadata": {
        "id": "aiqWEDxX9c50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, score = evaluate_model(X_train, y_train, X_test, y_test, 2, 128, 0.5, 2, 128, 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA-Jz7HW9hKs",
        "outputId": "e161c2c4-6fb2-44e0-f9a5-44cb80ad6aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_16 (LSTM)              (None, 72, 128)           69120     \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 72, 128)           0         \n",
            "                                                                 \n",
            " lstm_17 (LSTM)              (None, 128)               131584    \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 234,115\n",
            "Trainable params: 234,115\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "def plotModel():\n",
        "\n",
        "\tmodel = Sequential() # Create a Sequential model\n",
        "\tfor i in range(1, 2): # Add specified number of LSTM layers with specified number of neurons and input shape\n",
        "\t\tmodel.add(LSTM(units=128,return_sequences=True))\n",
        "\t\tmodel.add(Dropout(0.5)) # Add a dropout layer with specified rate\n",
        "\tmodel.add(LSTM(units=128, input_shape=(144,6)))\n",
        "\tmodel.add(Dropout(0.5)) # Add a dropout layer with specified rate\n",
        "\tfor i in range(2): # Add the specified number of dense layers with specified number of neurons and ReLU activation\n",
        "\t\tmodel.add(Dense(units=128, activation='relu'))\n",
        "\tmodel.add(Dense(units=2, activation='softmax')) # Add a dense output layer with softmax activation\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Compile the model with categorical crossentropy loss, Adam optimizer, and accuracy metric\n",
        "\tmodel.build(input_shape=(144,6))\n",
        "\tplot_model(model, to_file='acc.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# plotModel()"
      ],
      "metadata": {
        "id": "HowZb7LXITz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The accuracy of the model is: \", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_PcQmca9nPv",
        "outputId": "cac06c23-3a1e-4f56-edc1-1312a3f00b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the model is:  0.6486486196517944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Making predictions on a sequence of *videos*"
      ],
      "metadata": {
        "id": "FgEnoKJD9wUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_JOINTS_INTEREST = 6\n",
        "MAX_FRAMES_PER_VIDEO = 72\n",
        "def predict_sequence(model, input_sequence):\n",
        "    out_seq = []\n",
        "    for input in input_sequence:\n",
        "        out_seq.append(np.argmax(model.predict(input.reshape(1, MAX_FRAMES_PER_VIDEO, NUM_JOINTS_INTEREST), verbose = 0)))\n",
        "    return out_seq"
      ],
      "metadata": {
        "id": "JaK-Ivk2916D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(model, X, Y):\n",
        "    pred_values = []\n",
        "    for x in X:\n",
        "        pred_values.append(predict_sequence(model, x))\n",
        "    num_correct_preds = 0\n",
        "    # print(pred_values)\n",
        "    pred_values_ar = np.array(pred_values)\n",
        "    for idx in range(len(pred_values)):\n",
        "        if np.array_equal(pred_values_ar[idx],Y[idx]):\n",
        "            num_correct_preds += 1\n",
        "    print(\"The Sequential accuracy of model is: \", num_correct_preds/len(pred_values))\n",
        "    print(\"The Partial Accuracy of Model is: \", np.mean(np.equal(pred_values_ar, np.array(Y)))) #Accuracy of model considering even if one of the gestures is correctly predicted in the sequence."
      ],
      "metadata": {
        "id": "FUyHOknH-nZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the already saved concatenated videos for testing\n",
        "X_test = np.load(\"/content/drive/MyDrive/CS256Project/Milestone3/Combined_video_train_test/X_test.npz\", allow_pickle=True)['arr']\n",
        "Y_test = np.load(\"/content/drive/MyDrive/CS256Project/Milestone3/Combined_video_train_test/y_test.npz\", allow_pickle=True)['arr']"
      ],
      "metadata": {
        "id": "XOd4lFN7_gK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing the function for a random sample sequence\n",
        "predict_sequence(model, X_test[9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGu_gQXQ-rQq",
        "outputId": "a5456f81-7c6e-42e8-96fd-5bd8dddcd978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting Accuracy for first 500 concatenated videos in test dataset\n",
        "calc_accuracy(model, X_test[0:500], Y_test[0:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyQOnSTaAKLb",
        "outputId": "db6c38e2-b0d4-4287-d5a4-0738da610726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Sequential accuracy of model is:  0.43\n",
            "The Partial Accuracy of Model is:  0.677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDhE9H-tYebe"
      },
      "source": [
        "<a name=\"CNN-LSTM-model\"></a>\n",
        "## CNN-LSTM model classification performance\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.load(f\"{testpath}/X_train.npz\", allow_pickle=True)['arr']\n",
        "X_test = np.load(f\"{testpath}/X_test.npz\", allow_pickle=True)['arr']\n",
        "y_train = np.load(f\"{testpath}/y_train.npz\", allow_pickle=True)['arr']\n",
        "y_test = np.load(f\"{testpath}/y_test.npz\", allow_pickle=True)['arr']"
      ],
      "metadata": {
        "id": "_a0g8zh2Q61_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_cat = to_categorical(y_train.reshape(y_train.shape[0], y_train.shape[1], 1))\n",
        "y_test_cat = to_categorical(y_test.reshape(y_test.shape[0], y_test.shape[1], 1))"
      ],
      "metadata": {
        "id": "nRcsG_ygQ7WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train_cat.shape)\n",
        "print(y_test_cat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B9e9-wjwQJA",
        "outputId": "4f01ca7c-3f52-4a79-8501-443126ae6386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17833, 2, 72, 6)\n",
            "(4459, 2, 72, 6)\n",
            "(17833, 2, 3)\n",
            "(4459, 2, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import RepeatVector, TimeDistributed, Reshape, Conv1D, GlobalAveragePooling2D, MaxPooling1D\n",
        "from tensorflow.keras import Input"
      ],
      "metadata": {
        "id": "fdb9brHSTUZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_timesteps, n_features, n_outputs = X_train.shape[2], X_train.shape[3], y_train_cat.shape[-1] # Get the number of timesteps, features, and outputs\n",
        "\n",
        "model_s2s = Sequential()\n",
        "model_s2s.add(TimeDistributed(Conv1D(filters=72, kernel_size=3, activation='relu'), input_shape=(None,n_timesteps,n_features)))\n",
        "model_s2s.add(TimeDistributed(Conv1D(filters=72, kernel_size=3, activation='relu')))\n",
        "model_s2s.add(TimeDistributed(Dropout(0.5)))\n",
        "model_s2s.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "model_s2s.add(TimeDistributed(Flatten()))\n",
        "model_s2s.add(LSTM(128, activation='relu', return_sequences=True))\n",
        "model_s2s.add(Dropout(0.3))\n",
        "model_s2s.add(Dense(n_outputs, activation='softmax'))\n",
        "model_s2s.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Fa8Gf_eATWlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_s2s.summary()"
      ],
      "metadata": {
        "id": "sSVGjMyaTYcn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4deacd-1dc8-47b9-80ad-51f21fcb2bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " time_distributed (TimeDistr  (None, None, 70, 72)     1368      \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, None, 68, 72)     15624     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDis  (None, None, 68, 72)     0         \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, None, 34, 72)     0         \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_4 (TimeDis  (None, None, 2448)       0         \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 128)         1319424   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, None, 128)         0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 3)           387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,336,803\n",
            "Trainable params: 1,336,803\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(patience = 2, verbose = 1, monitor='val_accuracy',\n",
        "                           mode='max', min_delta=0.001, restore_best_weights = True)\n",
        "\n",
        "model_s2s.fit(X_train, y_train_cat, epochs=15, batch_size=32, verbose=1, validation_split = 0.2, callbacks=[early_stop]) # Fit the model to the training data"
      ],
      "metadata": {
        "id": "Q08GAamDTalM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3552f376-211c-4083-9f14-9529c536520c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "446/446 [==============================] - 45s 95ms/step - loss: 0.0982 - accuracy: 0.9601 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 2/15\n",
            "446/446 [==============================] - 35s 78ms/step - loss: 0.0091 - accuracy: 0.9974 - val_loss: 0.0046 - val_accuracy: 0.9996\n",
            "Epoch 3/15\n",
            "446/446 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9992Restoring model weights from the end of the best epoch: 1.\n",
            "446/446 [==============================] - 36s 82ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 1.3992e-04 - val_accuracy: 1.0000\n",
            "Epoch 3: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0e5ddda890>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy = model_s2s.evaluate(X_test, y_test_cat, batch_size=32, verbose=0)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "WCEk-ZMWTfxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264bc5fd-cfe5-4691-c7c0-25af72bbc9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9916666746139526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vvmwWOuTZLg"
      },
      "source": [
        "<a name=\"bconfusion matrix and classification report for analyzing the binary classification result\"></a>\n",
        "## confusion matrix and classification report for analyzing the binary classification result\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_labels(model, X):\n",
        "    pred = []\n",
        "    for x in X:\n",
        "        pred.append(np.argmax(model.predict(np.expand_dims(x, axis = 0), verbose = 0)[0], axis=1))\n",
        "    return np.array(pred)"
      ],
      "metadata": {
        "id": "YAG0bteeOF-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB4XEyZuTZLh"
      },
      "source": [
        "y_pred = predict_labels(model_s2s, X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "true_labels_flat = y_test.flatten()\n",
        "pred_labels_flat = y_pred.flatten()\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(true_labels_flat, pred_labels_flat)\n",
        "\n",
        "# Print the report\n",
        "print(report)\n",
        "\n",
        "cm = confusion_matrix(true_labels_flat, pred_labels_flat)\n",
        "\n",
        "#Confusion Matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(set(true_labels_flat)))\n",
        "disp.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "zqpWWpC-w3BY",
        "outputId": "9a19d991-9b0d-4dcc-dd44-deff80642490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99        78\n",
            "           1       1.00      0.97      0.99        79\n",
            "           2       1.00      1.00      1.00        83\n",
            "\n",
            "    accuracy                           0.99       240\n",
            "   macro avg       0.99      0.99      0.99       240\n",
            "weighted avg       0.99      0.99      0.99       240\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ff254f2cc70>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkRb3SeeTmQo"
      },
      "source": [
        "<a name=\"baseline-accuracy\"></a>\n",
        "## Best ML model and baseline accuracy discussion\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShBOwBkETmQo"
      },
      "source": [
        "### Approach 1. Train the model on each gesture independently. Use this model to predic a sequence of outputs by splitting the input into two separate videos.\n",
        "### However, this approach and it's accuracy were not satifactory."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGN3eCAWTmQo"
      },
      "source": [
        "### Approach 2. (Best ML Model) - The CNN-LSTM Seq2Seq Model, with time distribution and convolutional 1D.\n",
        "### The time distributed layer helps apply the same mathematical convolutional operations on the input file.\n",
        "### The shape of the input was (None, 2, 72, 6 ) and the same operations were performed on each 72 x 6 matrice for each video.\n",
        "### This reduces the dimensionality.\n",
        "\n",
        "\"\"\"\n",
        "model_s2s = Sequential()\n",
        "model_s2s.add(TimeDistributed(Conv1D(filters=72, kernel_size=3, activation='relu'), input_shape=(None,n_timesteps,n_features)))\n",
        "model_s2s.add(TimeDistributed(Conv1D(filters=72, kernel_size=3, activation='relu')))\n",
        "model_s2s.add(TimeDistributed(Dropout(0.5)))\n",
        "model_s2s.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "model_s2s.add(TimeDistributed(Flatten()))\n",
        "model_s2s.add(LSTM(128, activation='relu', return_sequences=True))\n",
        "model_s2s.add(Dropout(0.3))\n",
        "model_s2s.add(Dense(n_outputs, activation='softmax'))\n",
        "model_s2s.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoPIKxPBYebg"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXzcyssQTzNY"
      },
      "source": [
        "<a name=\"factor-summary\"></a>\n",
        "## Summary of what factor combination yields best prediction result\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdQcp3czTzNY"
      },
      "source": [
        "\"\"\"\n",
        "For the experimentation we tried the following hyper parameter combinations -\n",
        "Conv1D - experimented with tanh and relu activation functions.\n",
        "LSTM - numbers of neurons [32, 64, 128, 256], with 128 yielding the best results.\n",
        "We started with 2 LSTM layers, taking cues from Milestone 2, however after experimentation\n",
        "Single LSTM layer in combination with 2 Conv1D\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgxE1-OlT__P"
      },
      "source": [
        "<a name=\"result-discssion\"></a>\n",
        "## Refine/repeat/discussion of the results\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrQh775DT__Q"
      },
      "source": [
        "\"\"\"\n",
        "Experiments -\n",
        "\n",
        "TimeDistributed with LSTM:\n",
        "Apply LSTM to each time step independently using TimeDistributed.\n",
        "Captures temporal dependencies within each time step.\n",
        "Useful when modeling individual time step patterns.\n",
        "\n",
        "TimeDistributed with Conv1D:\n",
        "Apply convolution to each time step independently using TimeDistributed.\n",
        "Aggregates results across time steps for further processing.\n",
        "Useful for capturing local patterns and summarizing them.\n",
        "\n",
        "\n",
        "LSTM in TimeDistributed:\n",
        "Use TimeDistributed to wrap LSTM layer.\n",
        "Allows LSTM to process each time step independently.\n",
        "Useful for modeling temporal dependencies within each time step.\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yughIiLcT__Q"
      },
      "source": [
        "### Time Distributed with Conv1D yields the best results with test acuracy 100% when trained on the\n",
        "### combined videos dataset generated."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMBKS0IxVngW"
      },
      "source": [
        "\n",
        "# Bonus item B1 and B2\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLM5Gni3VazR"
      },
      "source": [
        "\"\"\"\n",
        "Colab Link: https://colab.research.google.com/drive/1Crg_-9QYl8q68fT5EQ3m_xY6C1apKqB1?usp=sharing\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}